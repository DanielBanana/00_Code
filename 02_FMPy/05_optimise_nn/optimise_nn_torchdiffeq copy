import argparse
import os
import math
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torchdiffeq import odeint_adjoint, odeint_event
from torch.autograd import Function
from torch.autograd.function import once_differentiable

import fmpy
from fmpy import read_model_description, extract
from fmpy.fmi2 import _FMU2 as FMU2
from fmpy.util import plot_result, download_test_file
import ctypes
from typing import List

import numpy as np
from collections import OrderedDict
from types import SimpleNamespace
import warnings

class FMUModule(nn.Module):
    fmu: FMU2

    def __init__(self, fmu, model_description, Tstart=0.0, Tend=1.0):
        super().__init__()
        self.fmu = fmu
        self.model_description = model_description
        self.initialized = False
        self.tnow = 0.0

        # Indicates if the adjoint ODE is currently being calculated or not
        # we don't need the FMU for that
        self.adjoint = False

        # Will be set in initialize_fmu()
        self.Tstart = Tstart
        self.Tend = Tend

        self._value_references = OrderedDict()
        self._input_value_references = OrderedDict()
        self._output_value_references = OrderedDict()
        self._state_value_references = OrderedDict()
        self._derivative_value_references = OrderedDict()
        for variable in model_description.modelVariables:
            self._value_references[variable.name] = variable.valueReference
            if variable.causality == "input":
                self._input_value_references[variable.name] = variable.valueReference
            if variable.causality == "output":
                self._output_value_references[variable.name] = variable.valueReference
            if variable.derivative:
                self._derivative_value_references[variable.name] = variable.valueReference
                self._state_value_references[
                    variable.derivative.name
                ] = variable.derivative.valueReference
        self._input_reference_numbers = list(self._input_value_references.values())
        self._output_reference_numbers = list(self._output_value_references.values())
        self._state_reference_numbers = list(self._state_value_references.values())
        self._derivative_reference_numbers = list(self._derivative_value_references.values())

        self.n_states = model_description.numberOfContinuousStates
        self.n_event_indicators = model_description.numberOfEventIndicators


        # Control the FMU simulation by setting parameters or determining
        # boundary values.
        self.physics_parameters = SimpleNamespace()

        # Example from the amesim code
        # self.physics_parameters = SimpleNamespace(switch_c=1)
        # self.physics_parameters.__dict__["mass_friction_endstops_1.xmin"] = 2.1
        # self.physics_parameters.__dict__["mass_friction_endstops_1.xmax"] = 2.1

    @property
    def state(self):
        return torch.tensor(self.fmu.getReal(self._state_reference_numbers))

    @property
    def output(self):
        return torch.tensor(self.fmu.getReal(self._output_reference_numbers))

    # equivalent to function "f" in fmpy_masterfile_vdp_input
    def forward(self, input, state):
        # if self.training:
        #     dx, y, df_dz_at_t, df_du_at_t = FMUFunction.apply(
        #         input,
        #         state,
        #         self.fmu,
        #         self.tnow,
        #         self.pointers,
        #         self._input_reference_numbers,
        #         self._output_reference_numbers,
        #         self._derivative_reference_numbers,
        #         self._state_reference_numbers,
        #         self.training,
        #     )
        #     return dx, y, df_dz_at_t, df_du_at_t

        # else:
        #     dx, y = FMUFunction.apply(
        #         input,
        #         state,
        #         self.fmu,
        #         self.tnow,
        #         self.pointers,
        #         self._input_reference_numbers,
        #         self._output_reference_numbers,
        #         self._derivative_reference_numbers,
        #         self._state_reference_numbers,
        #         self.training,
        #     )
        #     return dx, y

        dx, y = evaluate_fmu(input, state, self.fmu, self.tnow, self.pointers, self._input_reference_numbers, self._output_reference_numbers)
        if self.training:
            df_dz = torch.zeros(len(self._derivative_reference_numbers) + len(self._output_reference_numbers), len(self._state_reference_numbers))
            df_du = torch.zeros(len(self._derivative_reference_numbers) + len(self._output_reference_numbers), len(self._input_reference_numbers))

            # Gradients of the function w.r.t. the ODE parameters (like position, velocity,...)
            for k in range(len(self._state_reference_numbers)):
                df_dz[:, k] = torch.tensor(
                    fmu.getDirectionalDerivative(self._derivative_reference_numbers + self._output_reference_numbers, [self._state_reference_numbers[k]], [1.0])
                )
            # Gradients w.r.t. the control/input parameters like the output of a NN
            for k in range(len(self._input_reference_numbers)):
                df_du[:, k] = torch.tensor(
                    fmu.getDirectionalDerivative(self._derivative_reference_numbers + self._output_reference_numbers, [self._input_reference_numbers[k]], [1.0])
                )
            return dx, y, df_dz, df_du
        return dx, y

    def initialize_fmu(self, Tstart=0.0, Tend=10.0, value_references=None, values=None):
        self.fmu.setupExperiment(startTime=Tstart, stopTime=Tend)
        if value_references:
            self.fmu.setReal(value_references, values)
        self.fmu.enterInitializationMode()
        # set the input start values at time = Tstart
        pass
        self.fmu.exitInitializationMode()

        self.initialEventMode = False
        self.enterEventMode = False
        self.timeEvent = False
        self.stateEvent = False
        self.previous_event_indicator = np.zeros(self.n_event_indicators)
        self.fmu.enterContinuousTimeMode()

        # pointers to exchange state and derivative vectors with FMU
        self.pointers = SimpleNamespace(
            x=np.zeros(self.n_states),
            dx=np.zeros(self.n_states),
        )
        self.pointers._px = self.pointers.x.ctypes.data_as(
            ctypes.POINTER(ctypes.c_double)
        )
        self.pointers._pdx = self.pointers.dx.ctypes.data_as(
            ctypes.POINTER(ctypes.c_double)
        )

        self.initialized = True

    def reset_fmu(self):
        self.fmu.reset()
        self.initialized = False


# Core of the combination of FMU and NN
# This is a special type of function which evaluates the FMU + augment model and returns
# the output and the gradient. For that it needs a forward and backward implementation
# This is the equivalent to the function f in the file fmpy_masterfile_vdp_input.py
# located at 02_FMPy/04_optimise_mu
class FMUFunction(Function):
    @staticmethod
    def forward(ctx, u, x, *meta):
        (fmu, tnow, pointers, input_reference, output_reference, derivative_references,
            state_references, training) = meta

        dx, y = evaluate_fmu(u, x, fmu, tnow, pointers, input_reference, output_reference)

        if training:

            df_dz = torch.zeros(len(derivative_references) + len(output_reference), len(state_references))
            df_du = torch.zeros(len(derivative_references) + len(output_reference), len(input_reference))

            # Gradients of the function w.r.t. the ODE parameters (like position, velocity,...)
            for k in range(len(state_references)):
                df_dz[:, k] = torch.tensor(
                    fmu.getDirectionalDerivative(derivative_references + output_reference, [state_references[k]], [1.0])
                )
            # Gradients w.r.t. the control/input parameters like the output of a NN
            for k in range(len(input_reference)):
                df_du[:, k] = torch.tensor(
                    fmu.getDirectionalDerivative(derivative_references + output_reference, [input_reference[k]], [1.0])
                )

            # These are for the gradients of the
            ctx.save_for_backward(df_dz, df_du)
            return dx, y, df_dz, df_du

        return dx, y

    # With the adjoint method we can't calculate the gradient grad_u and grad_x
    # step by step. We calculate the gradients by solving an adjoint problem
    @staticmethod
    @once_differentiable
    def backward(ctx, *grad_outputs):
        grad_dx, grad_y, _, __ = grad_outputs

        grad_u = grad_x = None
        grad_meta = tuple([None] * 8)
        J_dxy_x, J_dxy_u = ctx.saved_tensors

        grad_u = torch.matmul(J_dxy_u.mT, torch.cat((grad_dx, grad_y), 0))
        grad_x = torch.matmul(J_dxy_x.mT, torch.cat((grad_dx, grad_y), 0))

        return grad_u, grad_x, *grad_meta

def evaluate_fmu(u, x, fmu, tnow, pointers, input_reference, output_reference):
    fmu.enterEventMode()
    newDiscreteStatesNeeded = True
    while newDiscreteStatesNeeded:
        (
            newDiscreteStatesNeeded,
            terminateSimulation,
            nominalsOfContinuousStatesChanged,
            valuesOfContinuousStatesChanged,
            nextEventTimeDefined,
            nextEventTime,
        ) = fmu.newDiscreteStates()
    fmu.enterContinuousTimeMode()

    # apply state
    pointers.x[:] = x.detach().numpy()
    fmu.setContinuousStates(pointers._px, pointers.x.size)

    # apply input
    fmu.setReal(input_reference, u.detach().tolist())

    # get state derivative
    fmu.getDerivatives(pointers._pdx, pointers.dx.size)

    fmu.setTime(tnow)
    step_event, _ = fmu.completedIntegratorStep()
    y = torch.tensor(fmu.getReal(output_reference))
    dx = torch.from_numpy(pointers.dx.astype(np.float32))

    return dx, y

def instantiate_fmu(fmu_filename, Tstart, Tend):
    """Needs to be called before a FMUModule object can be created

    Parameters
    ----------
    fmu_filename : str
        The name of the FMU file
    Tstart : float
        Starting time for the FMU
    Tend : float
        Stopping time for the FMU

    Returns
    -------
    Tuple
        First element is the FMPy fmu object, second element is the model_description object
    """
    model_description = read_model_description(fmu_filename)
    # extract the FMU
    unzipdir = extract(fmu_filename)
    fmu = fmpy.fmi2.FMU2Model(guid=model_description.guid,
                    unzipDirectory=unzipdir,
                    modelIdentifier=model_description.modelExchange.modelIdentifier,
                    instanceName='instance1')
    # instantiate
    fmu.instantiate()
    return fmu, model_description


class HybridModel(nn.Module):
    def __init__(
        self,
        fmu_module: FMUModule,
        augment_module: nn.Module,
        dt: float,
        solver
    ):
        super().__init__()
        self.fmu_module = fmu_module
        self.augment_module = augment_module
        self.dt = dt
        self.Tstat = fmu_module.Tstart
        self.Tend = fmu_module.Tend
        self.solver = solver

    def forward(self, t, state):
        u = self.augment_module(state)
        self.fmu_module.tnow += self.dt
        return self.fmu_module(u, state)

    def simulate(self, z0, t):
        # x0 = torch.cat([self.inital_pos, self.initial_aug]).reshape(-1)
        solution = odeint_adjoint(self, z0, t, self.fmu_module)
        return solution

def fmu_odeint(func, z0, t, fmu_module, *, rtol=1e-7, atol=1e-9, method=None, options=None, event_fn=None):
    n_batches = 1
    n_steps = t.shape[0]
    # Container to store trajectory
    X = torch.empty(n_batches, n_steps, len(fmu_module._state_reference_numbers))
    Y = torch.empty(n_batches, n_steps, len(fmu_module._output_reference_numbers))
    df_dz_trajectory = []
    df_du_trajectory = []
    if not fmu_module.adjoint:
        fmu = fmu_module.fmu
        fmu_module.initialize_fmu(fmu_module.Tstart, fmu_module.Tend)
        fmu_module.fmu.setReal(fmu_module._state_reference_numbers, fmu_module.state.detach().numpy())
        fmu.getContinuousStates(fmu_module.pointers._px, fmu_module.pointers.x.size)
    X[0, 0, :] = fmu_module.state
    Y[0, 0, :] = fmu_module.output
    for i in range(len(t)-1):
        status = fmu.setTime(t[i])
        dt = t[i+1] - t[i]
        derivatives, outputs, df_dz_at_t, df_du_at_t = func(t[i], fmu_module.state)
        X[0, i+1, :] = X[0, i, :] + dt * derivatives
        Y[0, i+1, :] = outputs
        df_dz_trajectory.append(df_dz_at_t)
        df_du_trajectory.append(df_du_at_t)
        fmu_module.pointers.x += dt * fmu_module.pointers.dx
        status = fmu.setContinuousStates(fmu_module.pointers._px, fmu_module.pointers.x.size)

        # get event indicators at t = time
        # status = fmu.getEventIndicators(fmu_module.pointers._pz, fmu_module.pointers.z.size)
        # inform the model about an accepted step
        enterEventMode, terminateSimulation = fmu.completedIntegratorStep()
        if terminateSimulation:
            break
    return X, Y, torch.stack(df_dz_trajectory), torch.stack(df_du_trajectory)

def aug_odeint(func, x0, t, *, rtol=1e-7, atol=1e-9, method=None, options=None, event_fn=None):
    n_batches = 1
    n_steps = t.shape[0]
    # Container to store trajectory
    # X = torch.empty(n_batches, n_steps, len(fmu_module._state_reference_numbers))
    # Y = torch.empty(n_batches, n_steps, len(fmu_module._output_reference_numbers))
    df_dz_trajectory = []
    df_du_trajectory = []
    X = [x0]
    for i in range(len(t)-1):
        dt = t[i+1] - t[i]
        derivatives = func(t[i], X[i])
        X.append(X[i] + dt * derivatives)

    return X

# def odeint(func, fmu_module, augment_module, control, dt, augment_parameters):
#     n_batches = control.shape[0]
#     n_steps = control.shape[1]

#     # Container to store trajectory
#     X = torch.empty(n_batches, n_steps, len(fmu_module._state_reference_numbers))
#     Y = torch.empty(n_batches, n_steps, len(fmu_module._output_reference_numbers))
#     df_dz_trajectory = []
#     df_du_trajectory = []


#     X[batch, step, :] = fmu_module.state


#     # custom_parameters contains all parameters for the FMU, which we want to
#     # control from the outside. We collect them all now.
#     # First add the physics_parameters.
#     custom_parameters = OrderedDict(
#         [
#             (key, [value for _ in range(n_batches)])
#             for key, value in fmu_module.physics_parameters.__dict__.items()
#         ]
#     )

#     # Now we add the parameters from the augment_model; e.g. the mu parameter for VdP
#     for key, value in augment_parameters.items():
#         custom_parameters[key] = value

#     assert all([len(x_) == n_batches for x_ in custom_parameters.values()])
#     custom_parameter_value_references = [fmu_module._state_value_references[x_] for x_ in custom_parameters.keys()]

#     for batch in range(n_batches):
#         if not fmu_module.initialized:
#             fmu_module.initialize_fmu(fmu_module.Tstart, fmu_module.Tend, custom_parameter_value_references, custom_parameters)
#         x = fmu_module.state
#         y = fmu_module.output

#         for step in range(n_steps):
#             # if solver == 'euler':
#             # u = augment_module(x)
#             X[batch, step, :] = x
#             Y[batch, step, :] = y
#             if fmu_module.training:
#                 dx, y, df_dz_at_t, df_du_at_t = func(0.0, x)
#                 df_dz_trajectory.append(df_dz_at_t)
#                 df_du_trajectory.append(df_du_at_t)
#             else:
#                 dx, y = func(0.0, x)
#             x = x + dt*dx

#             fmu_module.pointers.x += dt * fmu_module.pointers.dx

#             status = fmu.setContinuousStates(fmu_module.pointers._px, fmu_module.pointers.x.size)

#             # get event indicators at t = time
#             # status = fmu.getEventIndicators(fmu_module.pointers._pz, fmu_module.pointers.z.size)

#             # inform the model about an accepted step
#             enterEventMode, terminateSimulation = fmu.completedIntegratorStep()
#         # terminate_fmu(self.fmu_module.fmu)
#     if fmu_module.training:
#         df_dz_trajectory = torch.tensor(df_dz_trajectory)
#         df_du_trajectory = torch.tensor(df_du_trajectory)
#         return Y, X, df_dz_trajectory, df_du_trajectory
#     else:
#         return Y, X

def odeint_adjoint(func, y0, t, fmu_module, *, rtol=1e-7, atol=1e-9, method=None, options=None, event_fn=None,
                   adjoint_rtol=None, adjoint_atol=None, adjoint_method=None, adjoint_options=None, adjoint_params=None):

    # We need this in order to access the variables inside this module,
    # since we have no other way of getting variables along the execution path.
    if adjoint_params is None and not isinstance(func, nn.Module):
        raise ValueError('func must be an instance of nn.Module to specify the adjoint parameters; alternatively they '
                         'can be specified explicitly via the `adjoint_params` argument. If there are no parameters '
                         'then it is allowable to set `adjoint_params=()`.')

    # Must come before _check_inputs as we don't want to use normalised input (in particular any changes to options)
    if adjoint_rtol is None:
        adjoint_rtol = rtol
    if adjoint_atol is None:
        adjoint_atol = atol
    if adjoint_method is None:
        adjoint_method = method

    if adjoint_method != method and options is not None and adjoint_options is None:
        raise ValueError("If `adjoint_method != method` then we cannot infer `adjoint_options` from `options`. So as "
                         "`options` has been passed then `adjoint_options` must be passed as well.")

    if adjoint_options is None:
        adjoint_options = {k: v for k, v in options.items() if k != "norm"} if options is not None else {}
    else:
        # Avoid in-place modifying a user-specified dict.
        adjoint_options = adjoint_options.copy()

    if adjoint_params is None:
        adjoint_params = tuple(find_parameters(func))
    else:
        adjoint_params = tuple(adjoint_params)  # in case adjoint_params is a generator.

    # Filter params that don't require gradients.
    oldlen_ = len(adjoint_params)
    adjoint_params = tuple(p for p in adjoint_params if p.requires_grad)
    if len(adjoint_params) != oldlen_:
        # Some params were excluded.
        # Issue a warning if a user-specified norm is specified.
        if 'norm' in adjoint_options and callable(adjoint_options['norm']):
            warnings.warn("An adjoint parameter was passed without requiring gradient. For efficiency this will be "
                          "excluded from the adjoint pass, and will not appear as a tensor in the adjoint norm.")

    ans = OdeintAdjointMethod.apply(func, y0, fmu_module, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol,
                                    adjoint_method, adjoint_options, False, *adjoint_params)
    return ans

class OdeintAdjointMethod(Function):
    @staticmethod
    def forward(ctx, func, x0, fmu_module, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol,
                                    adjoint_method, adjoint_options, t_requires_grad, *adjoint_params):
        ctx.func = func
        ctx.fmu_module = fmu_module
        ctx.adjoint_rtol = adjoint_rtol
        ctx.adjoint_atol = adjoint_atol
        ctx.adjoint_method = adjoint_method
        ctx.adjoint_options = adjoint_options
        ctx.t_requires_grad = t_requires_grad
        ctx.event_mode = event_fn is not None

        with torch.no_grad():
            X, Y, df_dz, df_du = fmu_odeint(func, x0, t, fmu_module, rtol=rtol, atol=atol, method=method, options=options, event_fn=event_fn)
            ctx.save_for_backward(torch.tensor(t), X, *adjoint_params)
        return X

    # @staticmethod
    # def backward(ctx, *grad_y):
    #     solution, *adjoint_parameters = ctx.saved_tensors
    #     adj = adjoint_parameters


    #     def adjoint_dynamics(adj, z, z_ref, t, augment_parameters, df_dz_at_t):
    #         g_res = g(z, z_ref, augment_parameters)
    #         dg_dz = torch.autograd.grad(g_res, z)
    #         d_adj = -df_dz_at_t.T @ adj - dg_dz
    #         return d_adj

    #     return
    @staticmethod
    def backward(ctx, *grad_y):
        with torch.no_grad():
            func = ctx.func
            adjoint_rtol = ctx.adjoint_rtol
            adjoint_atol = ctx.adjoint_atol
            adjoint_method = ctx.adjoint_method
            adjoint_options = ctx.adjoint_options
            t_requires_grad = ctx.t_requires_grad
            fmu_module = ctx.fmu_module

            # Backprop as if integrating up to event time.
            # Does NOT backpropagate through the event time.
            event_mode = ctx.event_mode
            if event_mode:
                t, y, event_t, *adjoint_params = ctx.saved_tensors
                _t = t
                t = torch.cat([t[0].reshape(-1), event_t.reshape(-1)])
                grad_y = grad_y[1]
            else:
                t, y, *adjoint_params = ctx.saved_tensors
                grad_y = grad_y[0]

            adjoint_params = tuple(adjoint_params)

            ##################################
            #      Set up initial state      #
            ##################################

            # # [-1] because y and grad_y are both of shape (len(t), *y0.shape)
            # aug_state = [torch.zeros((), dtype=y.dtype, device=y.device), y[-1], grad_y[-1]]  # vjp_t, y, vjp_y
            # aug_state.extend([torch.zeros_like(param) for param in adjoint_params])  # vjp_params

            ##################################
            #    Set up backward ODE func    #
            ##################################

            # TODO: use a nn.Module and call odeint_adjoint to implement higher order derivatives.
            def augmented_dynamics(t, y_aug):
                # Dynamics of the original system augmented with
                # the adjoint wrt y, and an integrator wrt t and args.
                y = y_aug[1]
                adj_y = y_aug[2]
                # ignore gradients wrt time and parameters

                with torch.enable_grad():
                    t_ = t.detach()
                    t = t_.requires_grad_(True)
                    y = y.detach().requires_grad_(True)

                    # If using an adaptive solver we don't want to waste time resolving dL/dt unless we need it (which
                    # doesn't necessarily even exist if there is piecewise structure in time), so turning off gradients
                    # wrt t here means we won't compute that if we don't need it.
                    func_eval = func(t if t_requires_grad else t_, y)

                    # Workaround for PyTorch bug #39784
                    _t = torch.as_strided(t, (), ())  # noqa
                    _y = torch.as_strided(y, (), ())  # noqa
                    _params = tuple(torch.as_strided(param, (), ()) for param in adjoint_params)  # noqa

                    vjp_t, vjp_y, *vjp_params = torch.autograd.grad(
                        func_eval, (t, y) + adjoint_params, -adj_y,
                        allow_unused=True, retain_graph=True
                    )

                # autograd.grad returns None if no gradient, set to zero.
                vjp_t = torch.zeros_like(t) if vjp_t is None else vjp_t
                vjp_y = torch.zeros_like(y) if vjp_y is None else vjp_y
                vjp_params = [torch.zeros_like(param) if vjp_param is None else vjp_param
                              for param, vjp_param in zip(adjoint_params, vjp_params)]

                return (vjp_t, func_eval, vjp_y, *vjp_params)

            # # Add adjoint callbacks
            # for callback_name, adjoint_callback_name in zip(_all_callback_names, _all_adjoint_callback_names):
            #     try:
            #         callback = getattr(func, adjoint_callback_name)
            #     except AttributeError:
            #         pass
            #     else:
            #         setattr(augmented_dynamics, callback_name, callback)

            ##################################
            #       Solve adjoint ODE        #
            ##################################

            if t_requires_grad:
                time_vjps = torch.empty(len(t), dtype=t.dtype, device=t.device)
            else:
                time_vjps = None
            for i in range(len(t) - 1, 0, -1):
                if t_requires_grad:
                    # Compute the effect of moving the current time measurement point.
                    # We don't compute this unless we need to, to save some computation.
                    func_eval = func(t[i], y[i])
                    dLd_cur_t = func_eval.reshape(-1).dot(grad_y[i].reshape(-1))
                    aug_state[0] -= dLd_cur_t
                    time_vjps[i] = dLd_cur_t

                # Run the augmented system backwards in time.
                fmu_module.adjoint = True
                aug_state = aug_odeint(
                    augmented_dynamics, tuple(aug_state),
                    t[i - 1:i + 1].flip(0),
                    rtol=adjoint_rtol, atol=adjoint_atol, method=adjoint_method, options=adjoint_options
                )
                aug_state = [a[1] for a in aug_state]  # extract just the t[i - 1] value
                aug_state[1] = y[i - 1]  # update to use our forward-pass estimate of the state
                aug_state[2] += grad_y[i - 1]  # update any gradients wrt state at this time point

            if t_requires_grad:
                time_vjps[0] = aug_state[0]

            # Only compute gradient wrt initial time when in event handling mode.
            if event_mode and t_requires_grad:
                time_vjps = torch.cat([time_vjps[0].reshape(-1), torch.zeros_like(_t[1:])])

            adj_y = aug_state[2]
            adj_params = aug_state[3:]

        return (None, None, adj_y, time_vjps, None, None, None, None, None, None, None, None, None, None, *adj_params)

def find_parameters(module):
    assert isinstance(module, nn.Module)
    # If called within DataParallel, parameters won't appear in module.parameters().
    if getattr(module, '_is_replica', False):

        def find_tensor_attributes(module):
            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v) and v.requires_grad]
            return tuples

        gen = module._named_members(get_members_fn=find_tensor_attributes)
        return [param for _, param in gen]
    else:
        return list(module.parameters())

def g(z, z_ref, ode_parameters):
    '''Calculates the inner part of the loss function.

    This function can either take individual floats for z
    and z_ref or whole numpy arrays'''
    return torch.sum(0.5 * (z_ref - z)**2, axis = 0)

def J(z, z_ref, augment_parameters):
    return torch.sum(g(z, z_ref, augment_parameters))

class VdP(nn.Module):
    """Manages the augment model for the Reference solution. This means it
    contains the real term for the Van der Pol Oscillator which is missing
    in the FMU model

    Args:
        nn (_type_): _description_
    """
    def __init__(self) -> None:
        super().__init__()
        self.mu = nn.Parameter(torch.ones((1)))

    def forward(self, U):
        return self.mu # * (1.0 - U[0]**2)*U[1]


if __name__ == '__main__':

    torch.manual_seed(0)

    torch.set_default_dtype(torch.float64)

    fmu_filename = 'Van_der_Pol_input.fmu'
    path = os.path.abspath(__file__)
    fmu_filename = '/'.join(path.split('/')[:-1]) + '/' + fmu_filename
    Tstart = 0.0
    Tend = 50.0
    nSteps = 2000
    dt = (Tend - Tstart)/(nSteps)
    t = np.linspace(Tstart, Tend, nSteps)
    solver = 'euler'
    z0 = torch.tensor([1.0, 0.0])

    fmu, model_description = instantiate_fmu(fmu_filename, Tstart, Tend)
    fmu_model = FMUModule(fmu, model_description, Tstart, Tend)
    fmu_model.eval()

    augment_model = VdP()
    augment_model.mu.data[0] = torch.tensor(8.53)
    dummy_value = torch.tensor([2.0])

    # Create a dummy input since the VDP module does not need to be trained
    # U = torch.zeros((len(dummy_value), Tspan.size, len(fmu_model._input_reference_numbers)))

    # Perform the reference run
    model = HybridModel(fmu_model, augment_model, dt, solver)
    fmu_model.train()

    with torch.no_grad():
        # Y, X = model(U, augment_parameters=OrderedDict(zip(["mu"], [x1_values])))
        Xref = model.simulate(z0, t)
    # Yref = Y.detach()
    # Xref = X.detach()

    model.fmu_module.reset_fmu()

    augment_model = nn.Sequential(
        nn.Linear(2, 10),
        nn.Tanh(),
        # nn.Linear(1, 10),
        nn.Linear(10, 10),
        nn.Tanh(),
        nn.Linear(10, 1)
    )

    model = HybridModel(fmu_model, augment_model, dt, solver)
    fmu_model.reference_solution = Xref
    Xsol = model.simulate(z0, t)
    loss = ((Xsol - Xref)).abs().mean()
    loss.backward()
